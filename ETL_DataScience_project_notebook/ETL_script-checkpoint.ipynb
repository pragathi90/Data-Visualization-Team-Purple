{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Project - Scraping of desired Data Scientist skills requested in Job            Listings across United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html.parser\n",
      "Requirement already satisfied: ply in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from html.parser) (3.11)\n",
      "Installing collected packages: html.parser\n",
      "Successfully installed html.parser\n",
      "Requirement already satisfied: pattern3 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: cherrypy in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pattern3) (18.1.1)\n",
      "Requirement already satisfied: pdfminer.six in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pattern3) (20181108)\n",
      "Requirement already satisfied: simplejson in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pattern3) (3.16.0)\n",
      "Requirement already satisfied: feedparser in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pattern3) (5.2.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pattern3) (4.7.1)\n",
      "Requirement already satisfied: pdfminer3k in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pattern3) (1.3.1)\n",
      "Requirement already satisfied: docx in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pattern3) (0.2.4)\n",
      "Requirement already satisfied: cheroot>=6.2.4 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from cherrypy->pattern3) (6.5.4)\n",
      "Requirement already satisfied: portend>=2.1.1 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from cherrypy->pattern3) (2.4)\n",
      "Requirement already satisfied: zc.lockfile in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from cherrypy->pattern3) (1.4)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from cherrypy->pattern3) (6.0.0)\n",
      "Requirement already satisfied: pywin32; sys_platform == \"win32\" in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from cherrypy->pattern3) (223)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern3) (2.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern3) (1.12.0)\n",
      "Requirement already satisfied: pycryptodome in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern3) (3.8.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from beautifulsoup4->pattern3) (1.8)\n",
      "Requirement already satisfied: pytest>=2.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pdfminer3k->pattern3) (4.3.1)\n",
      "Requirement already satisfied: ply>=3.4 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: Pillow>=2.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from docx->pattern3) (5.4.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from docx->pattern3) (4.3.2)\n",
      "Requirement already satisfied: backports.functools-lru-cache in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from cheroot>=6.2.4->cherrypy->pattern3) (1.5)\n",
      "Requirement already satisfied: tempora>=1.8 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from portend>=2.1.1->cherrypy->pattern3) (1.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from zc.lockfile->cherrypy->pattern3) (40.8.0)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest>=2.0->pdfminer3k->pattern3) (1.8.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest>=2.0->pdfminer3k->pattern3) (19.1.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest>=2.0->pdfminer3k->pattern3) (1.3.0)\n",
      "Requirement already satisfied: pluggy>=0.7 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest>=2.0->pdfminer3k->pattern3) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest>=2.0->pdfminer3k->pattern3) (0.4.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2018.9)\n",
      "Requirement already satisfied: jaraco.functools>=1.20 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2.0)\n",
      "Requirement already satisfied: pyLDAvis in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.10)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.24.2)\n",
      "Requirement already satisfied: wheel>=0.23.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.33.1)\n",
      "Requirement already satisfied: numexpr in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.6.9)\n",
      "Requirement already satisfied: funcy in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.12)\n",
      "Requirement already satisfied: pytest in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.3.1)\n",
      "Requirement already satisfied: joblib>=0.8.4 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.16.2)\n",
      "Requirement already satisfied: future in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.17.1)\n",
      "Requirement already satisfied: scipy>=0.18.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (1.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (40.8.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (19.1.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: pluggy>=0.7 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (0.9.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (6.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yc886c\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install html.parser\n",
    "import html.parser\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "!{sys.executable} -m pip install pattern3\n",
    "import pattern3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import psycopg2 as ps\n",
    "import psycopg2.extras\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "!{sys.executable} -m pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python SQL toolkit and Object Relational Mapper\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, inspect, func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read In Data Scientist Position data from CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>reviews</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Development Director</td>\n",
       "      <td>ALS TDI</td>\n",
       "      <td>Development Director\\nALS Therapy Development ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta, GA 30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An Ostentatiously-Excitable Principal Research...</td>\n",
       "      <td>The Hexagon Lavish</td>\n",
       "      <td>Job Description\\n\\n\"The road that leads to acc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Xpert Staffing</td>\n",
       "      <td>Growing company located in the Atlanta, GA are...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Operation HOPE</td>\n",
       "      <td>DEPARTMENT: Program OperationsPOSITION LOCATIO...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Atlanta, GA 30303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            position             company  \\\n",
       "0                               Development Director             ALS TDI   \n",
       "1  An Ostentatiously-Excitable Principal Research...  The Hexagon Lavish   \n",
       "2                                     Data Scientist      Xpert Staffing   \n",
       "3                                       Data Analyst      Operation HOPE   \n",
       "\n",
       "                                         description  reviews  \\\n",
       "0  Development Director\\nALS Therapy Development ...      NaN   \n",
       "1  Job Description\\n\\n\"The road that leads to acc...      NaN   \n",
       "2  Growing company located in the Atlanta, GA are...      NaN   \n",
       "3  DEPARTMENT: Program OperationsPOSITION LOCATIO...     44.0   \n",
       "\n",
       "             location  \n",
       "0  Atlanta, GA 30301   \n",
       "1         Atlanta, GA  \n",
       "2         Atlanta, GA  \n",
       "3  Atlanta, GA 30303   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#csv_file_all_data = \"../Resources/tstdata.csv\"\n",
    "csv_file_all_data = \"../Resources/alldata.csv\"\n",
    "DS_jobs_all_data_df = pd.read_csv(csv_file_all_data,index_col=None, na_values=['NA'],sep=',')\n",
    "DS_jobs_all_data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the \"reviews\" column from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Development Director</td>\n",
       "      <td>ALS TDI</td>\n",
       "      <td>Development Director\\nALS Therapy Development ...</td>\n",
       "      <td>Atlanta, GA 30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An Ostentatiously-Excitable Principal Research...</td>\n",
       "      <td>The Hexagon Lavish</td>\n",
       "      <td>Job Description\\n\\n\"The road that leads to acc...</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Xpert Staffing</td>\n",
       "      <td>Growing company located in the Atlanta, GA are...</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Operation HOPE</td>\n",
       "      <td>DEPARTMENT: Program OperationsPOSITION LOCATIO...</td>\n",
       "      <td>Atlanta, GA 30303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            position             company  \\\n",
       "0                               Development Director             ALS TDI   \n",
       "1  An Ostentatiously-Excitable Principal Research...  The Hexagon Lavish   \n",
       "2                                     Data Scientist      Xpert Staffing   \n",
       "3                                       Data Analyst      Operation HOPE   \n",
       "\n",
       "                                         description            location  \n",
       "0  Development Director\\nALS Therapy Development ...  Atlanta, GA 30301   \n",
       "1  Job Description\\n\\n\"The road that leads to acc...         Atlanta, GA  \n",
       "2  Growing company located in the Atlanta, GA are...         Atlanta, GA  \n",
       "3  DEPARTMENT: Program OperationsPOSITION LOCATIO...  Atlanta, GA 30303   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create new data with selected columns- position, company, description, location\n",
    "DS_jobs_df = DS_jobs_all_data_df[[\"position\", \"company\",\"description\",\"location\"]].copy()\n",
    "DS_jobs_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform location into City, State,  Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Development Director</td>\n",
       "      <td>ALS TDI</td>\n",
       "      <td>Development Director\\nALS Therapy Development ...</td>\n",
       "      <td>Atlanta, GA 30301</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td>30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An Ostentatiously-Excitable Principal Research...</td>\n",
       "      <td>The Hexagon Lavish</td>\n",
       "      <td>Job Description\\n\\n\"The road that leads to acc...</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Xpert Staffing</td>\n",
       "      <td>Growing company located in the Atlanta, GA are...</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Operation HOPE</td>\n",
       "      <td>DEPARTMENT: Program OperationsPOSITION LOCATIO...</td>\n",
       "      <td>Atlanta, GA 30303</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td>30303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            position             company  \\\n",
       "0                               Development Director             ALS TDI   \n",
       "1  An Ostentatiously-Excitable Principal Research...  The Hexagon Lavish   \n",
       "2                                     Data Scientist      Xpert Staffing   \n",
       "3                                       Data Analyst      Operation HOPE   \n",
       "\n",
       "                                         description            location  \\\n",
       "0  Development Director\\nALS Therapy Development ...  Atlanta, GA 30301    \n",
       "1  Job Description\\n\\n\"The road that leads to acc...         Atlanta, GA   \n",
       "2  Growing company located in the Atlanta, GA are...         Atlanta, GA   \n",
       "3  DEPARTMENT: Program OperationsPOSITION LOCATIO...  Atlanta, GA 30303    \n",
       "\n",
       "      city state      zip  \n",
       "0  Atlanta    GA   30301   \n",
       "1  Atlanta    GA           \n",
       "2  Atlanta    GA           \n",
       "3  Atlanta    GA   30303   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transformation Step: Split location into City, \"State Zip\"\n",
    "#CityStateZip_df = new_DS_jobs_all_data_df[\"location\"].str.extract('(?P<City>[A-Z ]{1,2}),(?P<State>[A-Z]*$) (?P<Zipcode>\\d{5}){1,1}', expand=True)                                     \n",
    "#CityStateZip_df = new_DS_jobs_all_data_df[\"location\"].str.extract('(?P<City>([A-Z ]\\S+){2,})', expand=True)\n",
    "DS_jobs_df['city'], DS_jobs_df['state']  = DS_jobs_df[\"location\"].str.split(', ',1).str\n",
    "DS_jobs_df['zip'] = DS_jobs_df['state'].str[2:]\n",
    "DS_jobs_df['state'] = DS_jobs_df['state'].str[:2]\n",
    "DS_jobs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop column location from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>company</th>\n",
       "      <th>description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Development Director</td>\n",
       "      <td>ALS TDI</td>\n",
       "      <td>Development Director\\nALS Therapy Development ...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td>30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An Ostentatiously-Excitable Principal Research...</td>\n",
       "      <td>The Hexagon Lavish</td>\n",
       "      <td>Job Description\\n\\n\"The road that leads to acc...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Xpert Staffing</td>\n",
       "      <td>Growing company located in the Atlanta, GA are...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Operation HOPE</td>\n",
       "      <td>DEPARTMENT: Program OperationsPOSITION LOCATIO...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td>30303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            position             company  \\\n",
       "0                               Development Director             ALS TDI   \n",
       "1  An Ostentatiously-Excitable Principal Research...  The Hexagon Lavish   \n",
       "2                                     Data Scientist      Xpert Staffing   \n",
       "3                                       Data Analyst      Operation HOPE   \n",
       "\n",
       "                                         description     city state      zip  \n",
       "0  Development Director\\nALS Therapy Development ...  Atlanta    GA   30301   \n",
       "1  Job Description\\n\\n\"The road that leads to acc...  Atlanta    GA           \n",
       "2  Growing company located in the Atlanta, GA are...  Atlanta    GA           \n",
       "3  DEPARTMENT: Program OperationsPOSITION LOCATIO...  Atlanta    GA   30303   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS_jobs_df.drop(columns=[\"location\"], inplace=True)\n",
    "DS_jobs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the DataScientist_DB for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are connected to -  ('PostgreSQL 11.4, compiled by Visual C++ build 1914, 64-bit',) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Connect to the DataScientist_DB\n",
    "try: \n",
    "    conn = ps.connect(user = \"postgres\", \n",
    "                                  password = \"firstCls1\", \n",
    "                                  host = \"localhost\", \n",
    "                                  port = \"5432\", \n",
    "                                  database = \"DataScientist_DB\") \n",
    " \n",
    "    cursor = conn.cursor()\n",
    "    # Print PostgreSQL version \n",
    "    cursor.execute(\"SELECT version();\") \n",
    "    record = cursor.fetchone() \n",
    "    print(\"You are connected to - \", record,\"\\n\") \n",
    "\n",
    "except (Exception, ps.Error) as error : \n",
    "    print (\"Error while connecting to PostgreSQL\", error) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DS table with position details from the new_DS_jobs_all_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'position,company,description,city,state,zip'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for each position in the dataframe load it into the ds_tbl table\n",
    "df_columns = list(DS_jobs_df)\n",
    "# create (col1,col2,...)\n",
    "columns = \",\".join(df_columns)\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VALUES(%s,%s,%s,%s,%s,%s)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create VALUES('%s', '%s\",...) one '%s' per column\n",
    "values = \"VALUES({})\".format(\",\".join([\"%s\" for _ in df_columns])) \n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INSERT INTO ds_tbl (position,company,description,city,state,zip) VALUES(%s,%s,%s,%s,%s,%s)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create INSERT INTO table (columns) VALUES('%s',...)\n",
    "table = \"ds_tbl\"\n",
    "insert_stmt = \"INSERT INTO {} ({}) {}\".format(table,columns,values)\n",
    "insert_stmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Batch Load of Data Scientist Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n",
    "ps.extras.execute_batch(cursor, insert_stmt, DS_jobs_df.values)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read In GEO Demographics data from CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATEFIPS</th>\n",
       "      <th>STATE</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>agi_stub</th>\n",
       "      <th>N1</th>\n",
       "      <th>MARS1</th>\n",
       "      <th>MARS2</th>\n",
       "      <th>MARS4</th>\n",
       "      <th>PREP</th>\n",
       "      <th>N2</th>\n",
       "      <th>...</th>\n",
       "      <th>N10300</th>\n",
       "      <th>A10300</th>\n",
       "      <th>N85530</th>\n",
       "      <th>A85530</th>\n",
       "      <th>N85300</th>\n",
       "      <th>A85300</th>\n",
       "      <th>N11901</th>\n",
       "      <th>A11901</th>\n",
       "      <th>N11902</th>\n",
       "      <th>A11902</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>850050</td>\n",
       "      <td>481840</td>\n",
       "      <td>115070</td>\n",
       "      <td>240450</td>\n",
       "      <td>479900</td>\n",
       "      <td>1401930</td>\n",
       "      <td>...</td>\n",
       "      <td>389850</td>\n",
       "      <td>324575</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62690</td>\n",
       "      <td>47433</td>\n",
       "      <td>744910</td>\n",
       "      <td>1964826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AL</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>491370</td>\n",
       "      <td>200750</td>\n",
       "      <td>150290</td>\n",
       "      <td>125560</td>\n",
       "      <td>281350</td>\n",
       "      <td>1016010</td>\n",
       "      <td>...</td>\n",
       "      <td>397110</td>\n",
       "      <td>950446</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70780</td>\n",
       "      <td>101969</td>\n",
       "      <td>413790</td>\n",
       "      <td>1177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>AL</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>259540</td>\n",
       "      <td>75820</td>\n",
       "      <td>142970</td>\n",
       "      <td>34070</td>\n",
       "      <td>156720</td>\n",
       "      <td>589190</td>\n",
       "      <td>...</td>\n",
       "      <td>250230</td>\n",
       "      <td>1319641</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62170</td>\n",
       "      <td>132373</td>\n",
       "      <td>192050</td>\n",
       "      <td>538160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AL</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>164840</td>\n",
       "      <td>26730</td>\n",
       "      <td>125410</td>\n",
       "      <td>10390</td>\n",
       "      <td>99750</td>\n",
       "      <td>423300</td>\n",
       "      <td>...</td>\n",
       "      <td>163580</td>\n",
       "      <td>1394913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45120</td>\n",
       "      <td>124048</td>\n",
       "      <td>115470</td>\n",
       "      <td>375882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AL</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>203650</td>\n",
       "      <td>18990</td>\n",
       "      <td>177070</td>\n",
       "      <td>5860</td>\n",
       "      <td>122670</td>\n",
       "      <td>565930</td>\n",
       "      <td>...</td>\n",
       "      <td>203050</td>\n",
       "      <td>3655700</td>\n",
       "      <td>610</td>\n",
       "      <td>135</td>\n",
       "      <td>270</td>\n",
       "      <td>66</td>\n",
       "      <td>81180</td>\n",
       "      <td>387298</td>\n",
       "      <td>114380</td>\n",
       "      <td>448442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATEFIPS STATE  zipcode  agi_stub      N1   MARS1   MARS2   MARS4    PREP  \\\n",
       "0          1    AL        0         1  850050  481840  115070  240450  479900   \n",
       "1          1    AL        0         2  491370  200750  150290  125560  281350   \n",
       "2          1    AL        0         3  259540   75820  142970   34070  156720   \n",
       "3          1    AL        0         4  164840   26730  125410   10390   99750   \n",
       "4          1    AL        0         5  203650   18990  177070    5860  122670   \n",
       "\n",
       "        N2  ...  N10300   A10300  N85530  A85530  N85300  A85300  N11901  \\\n",
       "0  1401930  ...  389850   324575       0       0       0       0   62690   \n",
       "1  1016010  ...  397110   950446       0       0       0       0   70780   \n",
       "2   589190  ...  250230  1319641       0       0       0       0   62170   \n",
       "3   423300  ...  163580  1394913       0       0       0       0   45120   \n",
       "4   565930  ...  203050  3655700     610     135     270      66   81180   \n",
       "\n",
       "   A11901  N11902   A11902  \n",
       "0   47433  744910  1964826  \n",
       "1  101969  413790  1177400  \n",
       "2  132373  192050   538160  \n",
       "3  124048  115470   375882  \n",
       "4  387298  114380   448442  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_csv_file_data = \"../Resources/14zpallagi.csv\"\n",
    "geo_df = pd.read_csv(geo_csv_file_data)\n",
    "geo_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GEO Data table with GEO details from the 2015 Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STATEFIPS,STATE,zipcode,agi_stub,N1,MARS1,MARS2,MARS4,PREP,N2,NUMDEP,TOTAL_VITA,VITA,TCE,A00100,N02650,A02650,N00200,A00200,N00300,A00300,N00600,A00600,N00650,A00650,N00700,A00700,N00900,A00900,N01000,A01000,N01400,A01400,N01700,A01700,SCHF,N02300,A02300,N02500,A02500,N26270,A26270,N02900,A02900,N03220,A03220,N03300,A03300,N03270,A03270,N03150,A03150,N03210,A03210,N03230,A03230,N03240,A03240,N04470,A04470,A00101,N18425,A18425,N18450,A18450,N18500,A18500,N18300,A18300,N19300,A19300,N19700,A19700,N04800,A04800,N05800,A05800,N09600,A09600,N05780,A05780,N07100,A07100,N07300,A07300,N07180,A07180,N07230,A07230,N07240,A07240,N07220,A07220,N07260,A07260,N09400,A09400,N85770,A85770,N85775,A85775,N09750,A09750,N10600,A10600,N59660,A59660,N59720,A59720,N11070,A11070,N10960,A10960,N11560,A11560,N06500,A06500,N10300,A10300,N85530,A85530,N85300,A85300,N11901,A11901,N11902,A11902'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for each position in the dataframe load it into the ds_tbl table\n",
    "geo_df_columns = list(geo_df)\n",
    "# create (col1,col2,...)\n",
    "geocolumns = \",\".join(geo_df_columns)\n",
    "geocolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create VALUES('%s', '%s\",...) one '%s' per column\n",
    "#geovalues = \"VALUES({})\".format(\",\".join([\"%s\" for _ in geocolumns])) \n",
    "geovalues = 'VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);'\n",
    "geovalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INSERT INTO geo_population_tbl (STATEFIPS,STATE,zipcode,agi_stub,N1,MARS1,MARS2,MARS4,PREP,N2,NUMDEP,TOTAL_VITA,VITA,TCE,A00100,N02650,A02650,N00200,A00200,N00300,A00300,N00600,A00600,N00650,A00650,N00700,A00700,N00900,A00900,N01000,A01000,N01400,A01400,N01700,A01700,SCHF,N02300,A02300,N02500,A02500,N26270,A26270,N02900,A02900,N03220,A03220,N03300,A03300,N03270,A03270,N03150,A03150,N03210,A03210,N03230,A03230,N03240,A03240,N04470,A04470,A00101,N18425,A18425,N18450,A18450,N18500,A18500,N18300,A18300,N19300,A19300,N19700,A19700,N04800,A04800,N05800,A05800,N09600,A09600,N05780,A05780,N07100,A07100,N07300,A07300,N07180,A07180,N07230,A07230,N07240,A07240,N07220,A07220,N07260,A07260,N09400,A09400,N85770,A85770,N85775,A85775,N09750,A09750,N10600,A10600,N59660,A59660,N59720,A59720,N11070,A11070,N10960,A10960,N11560,A11560,N06500,A06500,N10300,A10300,N85530,A85530,N85300,A85300,N11901,A11901,N11902,A11902) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create INSERT INTO table (columns) VALUES('%s',...)\n",
    "table = \"geo_population_tbl\"\n",
    "insert_stmt = \"INSERT INTO {} ({}) {}\".format(table,geocolumns,geovalues)\n",
    "insert_stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 'AL', 0, ..., 47433, 744910, 1964826],\n",
       "       [1, 'AL', 0, ..., 101969, 413790, 1177400],\n",
       "       [1, 'AL', 0, ..., 132373, 192050, 538160],\n",
       "       ...,\n",
       "       [56, 'WY', 99999, ..., 2052, 1780, 5872],\n",
       "       [56, 'WY', 99999, ..., 7835, 1630, 7139],\n",
       "       [56, 'WY', 99999, ..., 33184, 260, 38860]], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Batch Load of GEO Population Data into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.extras.execute_batch(cursor, insert_stmt, geo_df.values)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Extracting, Transforming and Loading Desired Data Scientist Skills"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Objective: Extract all the desired skills listed in each job description, then store them in the skills table. \n",
    "Approach: \n",
    "    Step1. Retrieve all the position ids into a list from the ds_tbl table, then set it as the first column in the                DS_jobs_df.\n",
    "    \n",
    "    Step2. Setup for text lemmatization, special character and contractions removal and natual language toolkit,(nltk)            then tokenize each word using parts-of-speech,(pos)tagging.\n",
    "    \n",
    "    Step2. For each job description;\n",
    "            a. Extract each desired skill. \n",
    "            b. Store each skill and corresponding position id in the job_skills_df.\n",
    "            c. After all the job descriptions have been scraped, load the resulting job_skills_df into the skills_tbl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL - Retrieve position ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos_id\n",
       "0  202527\n",
       "1  202528\n",
       "2  202529\n",
       "3  202530"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_id_df = pd.read_sql_query('select pos_id from ds_tbl',conn)\n",
    "pos_id_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove EOL/CRLF characters from descriptions resulting in one string for scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Development Director ALS Therapy Development I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Job Description \"The road that leads to accomp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Growing company located in the Atlanta, GA are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEPARTMENT: Program OperationsPOSITION LOCATIO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0  Development Director ALS Therapy Development I...\n",
       "1  Job Description \"The road that leads to accomp...\n",
       "2  Growing company located in the Atlanta, GA are...\n",
       "3  DEPARTMENT: Program OperationsPOSITION LOCATIO..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_df = pd.read_sql_query(\"select regexp_replace(description, E'[\\\\n\\\\r]+', ' ', 'g' ) as description from ds_tbl\",conn)\n",
    "desc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate position ids list as the first column in the DS_jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_id</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202527</td>\n",
       "      <td>Development Director ALS Therapy Development I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202528</td>\n",
       "      <td>Job Description \"The road that leads to accomp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202529</td>\n",
       "      <td>Growing company located in the Atlanta, GA are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202530</td>\n",
       "      <td>DEPARTMENT: Program OperationsPOSITION LOCATIO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos_id                                        description\n",
       "0  202527  Development Director ALS Therapy Development I...\n",
       "1  202528  Job Description \"The road that leads to accomp...\n",
       "2  202529  Growing company located in the Atlanta, GA are...\n",
       "3  202530  DEPARTMENT: Program OperationsPOSITION LOCATIO..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_desc_df = pd.concat((pos_id_df, desc_df), axis='columns')\n",
    "id_desc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set discription field values to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_id</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202527</td>\n",
       "      <td>development director als therapy development i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202528</td>\n",
       "      <td>job description \"the road that leads to accomp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202529</td>\n",
       "      <td>growing company located in the atlanta, ga are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202530</td>\n",
       "      <td>department: program operationsposition locatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos_id                                        description\n",
       "0  202527  development director als therapy development i...\n",
       "1  202528  job description \"the road that leads to accomp...\n",
       "2  202529  growing company located in the atlanta, ga are...\n",
       "3  202530  department: program operationsposition locatio..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_desc_df['description'] = id_desc_df['description'].str.lower() \n",
    "id_desc_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Natural Language Processing to Scrape Data Scienctist Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by defining Data Scienctist Skills (for future development, implement Machine Learning to identify skills learning from all the previous skills identified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actionable recommendations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adapt quickly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alteryx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>analytical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analytical tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>analyze data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>audits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>azure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b2b applications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bachelor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bachelor's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>best practices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>business intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>business team members</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>capture stakeholders feedback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>caseworthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>client</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>client service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cloud based tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cloud-based architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>collect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>collecting data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>collects</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           skills\n",
       "0      actionable recommendations\n",
       "1                   adapt quickly\n",
       "2                         alteryx\n",
       "3                      analytical\n",
       "4                analytical tools\n",
       "5                    analyze data\n",
       "6                         audits \n",
       "7                           azure\n",
       "8                b2b applications\n",
       "9                        bachelor\n",
       "10                     bachelor's\n",
       "11                 best practices\n",
       "12          business intelligence\n",
       "13          business team members\n",
       "14  capture stakeholders feedback\n",
       "15                           case\n",
       "16                     caseworthy\n",
       "17                          clean\n",
       "18                         client\n",
       "19                 client service\n",
       "20              cloud based tools\n",
       "21       cloud-based architecture\n",
       "22                        collect\n",
       "23                collecting data\n",
       "24                       collects"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skillsfile = \"../Resources/skills_list.csv\"\n",
    "skills_match_list_df = pd.read_csv(skillsfile)\n",
    "skills_match_list_df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate Natural Language Processing to Scrape Data Scienctist Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "html_parser = HTMLParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the dictionary of word contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the list of stopwords from NLTK and amend it by adding more stopwords to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get',\n",
    "                                 'tell', 'listen', 'one', 'two', 'three',\n",
    "                                 'four', 'five', 'six', 'seven', 'eight',\n",
    "                                 'nine', 'zero', 'join', 'find', 'make',\n",
    "                                 'say', 'ask', 'tell', 'see', 'try', 'back',\n",
    "                                 'also','would']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text into word tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(position_desc):\n",
    "    tokens = nltk.word_tokenize(position_desc) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "    text = tokenize_text(position_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    " \n",
    "    text = expand_contractions(text, contraction_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate text tokens with Part-Of-Speach tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_text(text_tokens):\n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None  \n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text\n",
    "\n",
    "    text = pos_tag_text(tokenize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize text based on Part-Of-Speech (POS) tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    " \n",
    "    text = lemmatize_text(tokenize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove special characters, such as punctuation marks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text \n",
    "\n",
    "    text = remove_special_characters(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "    text = remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all non-text characters (numbers, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "    text = keep_text_characters(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up HTML markups: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ' '.join(self.fed)\n",
    "    \n",
    "def strip_html(text):\n",
    "    html_stripper = MLStripper()\n",
    "    html_stripper.feed(text)\n",
    "    return html_stripper.get_data()\n",
    "\n",
    "    text = strip_html(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing accents from characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_accented_characters(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8')\n",
    "    return text\n",
    "\n",
    "    text = normalize_accented_characters(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the lemma tokenized words try to match each word to the skills_match_list df to identify desired Data Scientist skills to load to the skills table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_skills(text):\n",
    "    skills_list = []\n",
    "    \n",
    "    #Compare each word in the normalized position description to skills list: if matched append skill to skills_list\n",
    "    for word_tokens in text:\n",
    "        isSkill = word_tokens in skills_match_list_df.values\n",
    "        if isSkill:\n",
    "            skills_list.append(word_tokens)\n",
    "            isSkill = False\n",
    "    \n",
    "    #Remove duplicate skills from list\n",
    "    normalize_skills_list = list(set(skills_list))\n",
    "    \n",
    "    #return normalized_skills_list\n",
    "    return normalize_skills_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main process loop through all position descriptions to scrape all desired skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_skills_list(corpus, only_text_chars=True):\n",
    "\n",
    "    normalized_skills_list = []\n",
    "    index = 0\n",
    "    #for text in corpus_desc:\n",
    "    index += 1 \n",
    "    text = normalize_accented_characters(corpus)\n",
    "    text = html.unescape(text)\n",
    "    text = strip_html(text)\n",
    "    text = expand_contractions(text, contraction_mapping)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = tokenize_text(text)\n",
    "    print(text)\n",
    "    \n",
    "    normalized_skills_list = identify_skills(text)    \n",
    "    return normalized_skills_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_skills(hld_pos_id, skills_list):\n",
    "    skills_list_df = pd.DataFrame(skills_list, columns = ['skill'])\n",
    "    skills_list_df['pos_id']=hld_pos_id\n",
    "        \n",
    "    print(\"skill list df: \", skills_list_df)\n",
    "    \n",
    "    #Generate the sql insert statement to load all the skills identifed for the current DS position\n",
    "    insert_stmt = \"INSERT INTO skills_tbl (skill, pos_id) VALUES(%s, %s);\"\n",
    "    \n",
    "    #Load skills into the skills database table\n",
    "    ps.extras.execute_batch(cursor, insert_stmt, skills_list_df.values)\n",
    "    conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['development', 'director', 'als', 'therapy', 'development', 'institute', 'immediate', 'opening', 'development', 'directors', 'reporting', 'directly', 'senior', 'development', 'director', 'development', 'director', 'als', 'tdi', 'senior', 'fundraising', 'position', 'working', 'identifying', 'potential', 'prospects', 'cultivating', 'solicitation', 'strategies', 'closing', 'asks', 'donors', 'including', 'individuals', 'corporations', 'building', 'networks', 'via', 'events', 'generating', 'awareness', 'als', 'tdi', 'outreach', 'including', 'attending', 'speaking', 'events', 'well', 'personally', 'cultivates', 'relationships', 'patients', 'prospects', 'donors', 'position', 'responsible', 'generating', 'managing', 'portfolio', 'least', 'million', 'million', 'dollars', 'per', 'year', 'position', 'located', 'atlanta', 'ga', 'requirements', 'bachelors', 'degree', 'requiredminimum', '6', '8', 'years', 'experience', 'fundraising', 'business', 'developmentsuccessful', 'track', 'recording', 'fundraising', 'major', 'donors', 'scientific', 'sales', 'preferreddemonstrated', 'ability', 'work', 'independently', 'progress', 'several', 'prospects', 'projects', 'time', 'excellent', 'english', 'oral', 'written', 'presentation', 'skillsstrong', 'leadership', 'management', 'skills', 'well', 'ability', 'forge', 'strong', 'interpersonal', 'relationshipsability', 'travelexcellent', 'computer', 'skills', 'including', 'use', 'word', 'processing', 'spreadsheet', 'database', 'presentation', 'prospect', 'management', 'software', 'operating', 'pc', 'based', 'system', 'including', 'salesforce', 'als', 'therapy', 'development', 'institute', 'als', 'net', 'als', 'therapy', 'development', 'institute', 'als', 'net', 'scientists', 'actively', 'discover', 'develop', 'treatments', 'als', 'als', 'net', 'worlds', 'first', 'largest', 'nonprofit', 'biotech', 'focused', '100', 'als', 'research', 'led', 'als', 'patients', 'families', 'charity', 'understands', 'urgent', 'need', 'slow', 'stop', 'horrible', 'disease', 'als', 'net', 'based', 'cambridge', 'served', 'leaders', 'sharing', 'data', 'information', 'academic', 'als', 'research', 'organizations', 'patients', 'families', 'information', 'visit', 'www', 'als', 'net', 'apply', 'please', 'apply', 'https', 'als', 'applicantpro', 'com', 'please', 'include', 'cover', 'letter', 'salary', 'requirements', 'resume', 'phone', 'calls', 'please', 'als', 'tdi', 'equal', 'opportunity', 'employer']\n",
      "skill list df:           skill  pos_id\n",
      "0     database  202527\n",
      "1        sales  202527\n",
      "2  development  202527\n",
      "3    reporting  202527\n",
      "4    excellent  202527\n",
      "5  responsible  202527\n",
      "6  information  202527\n",
      "7   management  202527\n",
      "['database', 'sales', 'development', 'reporting', 'excellent', 'responsible', 'information', 'management']\n",
      "['job', 'description', 'road', 'leads', 'accomplishment', 'narrow', 'desmond', 'ostentatious', 'strong', 'adjective', 'fits', 'description', 'individual', 'inhibits', 'natural', 'innate', 'spirit', 'vivaciousness', 'vigor', 'eager', 'characteristics', 'comprise', 'disposition', 'willing', 'learn', 'take', 'attitude', 'eternal', 'student', 'said', 'atlanta', 'georgia', 'based', 'scientific', 'r', 'outfit', 'hexagon', 'lavish', 'open', 'position', 'excitable', 'principal', 'research', 'assistant', 'mentored', 'guided', 'chief', 'science', 'officer', 'position', 'includes', 'computational', 'statistical', 'analysis', 'sequencing', 'studies', 'technical', 'logistical', 'collaboration', 'international', 'consortia', 'well', 'excitable', 'principal', 'research', 'assistant', 'responsible', 'setting', 'schedule', 'entry', 'level', 'scientists', 'guided', 'chief', 'scientist', 'excitable', 'principal', 'research', 'assistant', 'charge', 'delegating', 'reports', 'responsibilities', 'project', 'leads', 'behalf', 'chief', 'science', 'officer', 'chief', 'technology', 'officer', 'excitable', 'principal', 'research', 'assistant', 'engage', 'entry', 'level', 'scientists', 'researchers', 'setting', 'work', 'schedules', 'entry', 'level', 'scientists', 'researchers', 'heavily', 'involved', 'workshops', 'internal', 'team', 'seminars', 'well', 'perform', 'clerical', 'administrative', 'duties', 'reports', 'directly', 'addressed', 'chief', 'science', 'officer', 'solid', 'background', 'constructive', 'optics', 'algorithms', 'combined', 'expansive', 'computer', 'vision', 'image', 'inferential', 'preferable', 'candidacy', 'qualifications', 'integrating', 'data', 'across', 'formats', 'sources', 'robust', 'synchronization', 'existing', 'datasets', 'databases', 'general', 'direction', 'developing', 'innovative', 'analytical', 'methods', 'enable', 'collaborators', 'interpret', 'design', 'follow', 'research', 'providing', 'support', 'internal', 'team', 'members', 'across', 'interdisciplinary', 'backgrounds', 'lead', 'contribute', 'manuscript', 'preparation', 'well', 'internal', 'external', 'project', 'team', 'reports', 'actively', 'participating', 'project', 'team', 'group', 'meetings', 'additional', 'information', 'essential', 'job', 'functions', 'ph', 'computer', 'science', 'mathematics', 'statistics', 'chemistry', 'quantitative', 'discipline', 'required', '5', 'years', 'experience', 'varying', 'datasets', 'experience', 'computational', 'biology', 'statistics', 'physics', 'mathematics', 'computer', 'science', 'algorithms', 'incorporate', 'learning', 'etc', 'relevant', 'experience', 'formal', 'training', 'laboratorial', 'clinical', 'research', 'must', 'demonstrable', 'proficiency', 'several', 'standard', 'computational', 'languages', 'form', 'esoteric', 'programming', 'familiarity', 'image', 'inferential', 'analysis', 'tools', 'familiarity', 'range', 'optics', 'algorithm', 'tools', 'thorough', 'understanding', 'statistics', 'preferred', 'application', 'modeling', 'software', 'expertise', 'analysis', 'complex', 'r', 'efforts', 'conduct', 'feasibility', 'studies', 'provide', 'technical', 'assessment', 'proposed', 'research', 'preparation', 'briefing', 'materials', 'research', 'announcements', 'technical', 'reports', 'technical', 'monitoring', 'r', 'efforts', 'adherence', 'statement', 'work', 'sow', 'tasks', 'program', 'milestones', 'deliverables', 'participation', 'program', 'reviews', 'site', 'visits', 'field', 'tests', 'coordination', 'scientific', 'workshops', 'required', 'summarize', 'communicate', 'intellectual', 'content', 'scientific', 'technological', 'information', 'scientific', 'non', 'scientific', 'audiences', 'interact', 'stakeholders', 'multiple', 'disciplines', 'including', 'engineering', 'biology', 'neuroscience', 'clinical', 'regulatory', 'policy', 'teams', 'demonstrable', 'mastery', 'high', 'performance', 'computing', 'low', 'level', 'high', 'performance', 'development', 'skills', 'needed', 'perform', 'research', 'distributed', 'highly', 'scalable', 'architectures', 'conducive', 'evolving', 'nature', 'pir', 'essential', 'job', 'functions', 'sound', 'decision', 'maker', 'establish', 'effectiveness', 'mentor', 'contributory', 'scientific', 'endeavors', 'company', 'skills', 'abilities', 'excellent', 'communication', 'skills', 'verbally', 'written', 'form', 'fluent', 'english', 'strong', 'interpersonal', 'skills', 'must', 'demonstrate', 'high', 'degree', 'integrity', 'trustworthiness', 'respect', 'others', 'ability', 'deal', 'appropriately', 'confidential', 'sensitive', 'information', 'must', 'demonstrate', 'understanding', 'acceptance', 'hexagon', 'lavishs', 'mission', 'vision', 'values', 'hexagon', 'lavish', 'equal', 'opportunity', 'employer']\n",
      "skill list df:            skill  pos_id\n",
      "0   statistical  202528\n",
      "1    technology  202528\n",
      "2    developing  202528\n",
      "3    analytical  202528\n",
      "4   development  202528\n",
      "5      modeling  202528\n",
      "6             r  202528\n",
      "7     excellent  202528\n",
      "8     integrity  202528\n",
      "9       methods  202528\n",
      "10  performance  202528\n",
      "11  responsible  202528\n",
      "12       design  202528\n",
      "13  information  202528\n",
      "14     datasets  202528\n",
      "15    databases  202528\n",
      "16   innovative  202528\n",
      "['statistical', 'technology', 'developing', 'analytical', 'development', 'modeling', 'r', 'excellent', 'integrity', 'methods', 'performance', 'responsible', 'design', 'information', 'datasets', 'databases', 'innovative']\n",
      "['growing', 'company', 'located', 'atlanta', 'ga', 'area', 'currently', 'looking', 'add', 'data', 'scientist', 'team', 'data', 'scientist', 'analyze', 'business', 'level', 'data', 'produce', 'actionable', 'insights', 'utilizing', 'analytics', 'tools', 'languages', 'etc', 'r', 'python', 'c', 'data', 'scientist', 'serve', 'organizations', 'leader', 'helping', 'grow', 'data', 'science', 'initiative', 'green', 'state', 'data', 'scientist', 'responsible', 'advancement', 'analytical', 'projects', 'inception', 'delivery', 'beyond', 'responsiblities', '1', 'leverage', 'big', 'data', 'discover', 'patterns', 'solve', 'strategic', 'tactical', 'business', 'problems', 'using', 'massive', 'structured', 'unstructured', 'data', 'sets', 'across', 'multiple', 'environments', '2', 'develop', 'analytical', 'capabilities', 'modeling', 'processes', 'drive', 'better', 'outcomes', 'customers', 'company', '3', 'drive', 'collection', 'cleansing', 'processing', 'analysis', 'new', 'existing', 'data', 'sources', '4', 'research', 'industry', 'topics', 'impacting', 'opportunities', 'relevant', 'data', 'analysis', 'projects', '5', 'execute', 'complex', 'analyses', 'aid', 'reporting', 'interpretation', 'analytical', 'findings', 'build', 'comprehensive', 'solution', '6', 'demonstrates', 'good', 'judgment', 'analytical', 'skills', 'conduct', 'option', 'analysis', 'present', 'recommendations', '7', 'support', 'resolve', 'issues', 'related', 'analyses', 'deliverable', 'production', '8', 'works', 'internal', 'external', 'clients', 'understand', 'clarify', 'analyze', 'requirements', 'understand', 'business', 'potential', 'impactful', 'solutions', '9', 'mapping', 'processes', 'understand', 'opportunities', 'advanced', 'analytics', 'product', 'enhancement', 'requirements', '5', '1', 'ba', 'bs', 'degree', 'business', 'analysis', 'computer', 'science', 'economics', 'data', 'science', 'masters', 'degree', 'preferred', '2', '5', 'years', 'predictive', 'analytics', 'statistical', 'modeling', 'machine', 'learning', 'plus', '3', 'expect', 'level', 'understanding', 'analytics', 'tools', 'language', 'r', 'python', 'c', 'etc', '4', 'experienced', 'business', 'analysis', 'requirement', 'definition', '5', 'proven', 'recent', 'experience', 'working', 'sales', 'marketing', 'supply', 'chain', 'manufacturing', 'data', '6', 'able', 'navigate', 'access', 'structured', 'unstructured', 'data', 'environments', 'e', 'hadoop', 'oracle', 'sql', 'etc', '7', 'experience', 'utilizing', 'internal', 'external', 'data', 'sources', 'identify', 'otherwise', 'hidden', 'industry', 'trends', 'impact', 'product', 'development', '8', 'exposure', 'visualization', 'tools', 'tableau', 'qliksense', '9', 'must', 'solution', 'oriented', 'focused', 'innovation', '10', 'familiar', 'scripting', 'language', 'e', 'g', 'perl', 'python', 'programming', 'language', 'e', 'g', 'java', 'express', 'interest', 'position', 'please', 'email', 'resume', 'ms', 'word', 'pdf', 'format', 'attach', 'cover', 'letter', 'salary', 'requirements', 'contact', 'information', 'please', 'sure', 'reference', 'job', '2093', 'sponsorship', 'available', 'location', 'atlanta', 'ga', 'type', 'permanent', 'salary', 'doe']\n",
      "skill list df:            skill  pos_id\n",
      "0         sales  202529\n",
      "1   development  202529\n",
      "2           sql  202529\n",
      "3   information  202529\n",
      "4        python  202529\n",
      "5    analytical  202529\n",
      "6        issues  202529\n",
      "7        trends  202529\n",
      "8     marketing  202529\n",
      "9      findings  202529\n",
      "10    reporting  202529\n",
      "11      tableau  202529\n",
      "12  responsible  202529\n",
      "13     patterns  202529\n",
      "14  statistical  202529\n",
      "15     insights  202529\n",
      "16     modeling  202529\n",
      "17            r  202529\n",
      "18       hadoop  202529\n",
      "['sales', 'development', 'sql', 'information', 'python', 'analytical', 'issues', 'trends', 'marketing', 'findings', 'reporting', 'tableau', 'responsible', 'patterns', 'statistical', 'insights', 'modeling', 'r', 'hadoop']\n",
      "['department', 'program', 'operationsposition', 'locations', 'atlanta', 'ga', 'accountability', 'president', 'program', 'operationsfor', '25', 'years', 'operation', 'hope', 'dedicated', 'empowering', 'underserved', 'communities', 'financial', 'literacy', 'work', 'encompasses', 'financial', 'education', 'work', 'instilling', 'knowledge', 'confidence', 'serve', 'experience', 'sometimes', 'first', 'time', 'lives', 'financial', 'dignity', 'job', 'summarythe', 'data', 'analyst', 'collects', 'organizes', 'mines', 'analyzes', 'audits', 'large', 'sets', 'data', 'impact', 'reporting', 'operational', 'optimization', 'data', 'analyst', 'participates', 'team', 'member', 'responsible', 'maintaining', 'integrity', 'client', 'database', 'information', 'provides', 'executable', 'actionable', 'recommendations', 'data', 'migration', 'best', 'practices', 'field', 'operations', 'based', 'analytical', 'findings', 'duties', 'responsibilitiesinitiate', 'participate', 'data', 'mining', 'reportingaudit', 'profile', 'data', 'assess', 'impact', 'poor', 'quality', 'data', 'organizations', 'performance', 'impacts', 'conduct', 'b', 'testing', 'based', 'different', 'hypotheses', 'directly', 'indirectly', 'impact', 'operational', 'key', 'performance', 'indicators', 'create', 'support', 'data', 'visualization', 'operational', 'executive', 'dashboards', 'import', 'collect', 'clean', 'convert', 'analyze', 'data', 'purpose', 'insights', 'making', 'conclusions', 'design', 'develop', 'relational', 'databases', 'collecting', 'data', 'monitor', 'performance', 'data', 'systems', 'issues', 'respond', 'keep', 'track', 'trends', 'patterns', 'correlation', 'case', 'complex', 'data', 'sets', 'prepare', 'concise', 'data', 'reports', 'data', 'visualizations', 'management', 'help', 'decision', 'making', 'process', 'assist', 'data', 'scientist', 'development', 'new', 'analytical', 'tools', 'methods', 'required', 'required', 'skills', 'educationbachelors', 'degreetwo', 'years', 'data', 'analyst', 'related', 'fieldable', 'understand', 'various', 'data', 'structures', 'common', 'methods', 'data', 'transformationexperience', 'basic', 'statistical', 'modeling', 'reportingexperience', 'sql', 'relational', 'databases', 'database', 'concepts', 'dimensional', 'modeling', 'database', 'designproficient', 'programming', 'languages', 'r', 'python', 'stata', 'sasexperience', 'caseworthy', 'sql', 'tableaueeostatementoperation', 'hope', 'equal', 'opportunity', 'employer', 'qualified', 'applicants', 'receive', 'consideration', 'employment', 'without', 'regard', 'race', 'color', 'religion', 'sex', 'age', 'national', 'origin', 'protected', 'veteran', 'status', 'disability', 'status', 'sexual', 'orientation', 'gender', 'identity', 'expression', 'marital', 'status', 'genetic', 'information', 'characteristic', 'protected', 'law', 'job', 'type', 'full', 'timesalary', '50', '000', '00', '60', '000', '00', 'yearexperience', 'data', 'analyst', 'related', 'field', '2', 'yearsbasic', 'statistical', 'modeling', 'reporting', '2', 'yearsprogramming', 'languages', 'r', 'python', 'stata', 'sas', '2', 'yearssql', 'relational', 'databases', 'database', 'concepts', 'design', '2', 'yearscaseworthy', 'sql', 'tableau', '1', 'yeareducation', 'bachelorslocation', 'atlanta', 'ga']\n",
      "skill list df:             skill  pos_id\n",
      "0         client  202530\n",
      "1          stata  202530\n",
      "2   optimization  202530\n",
      "3           case  202530\n",
      "4    development  202530\n",
      "5      integrity  202530\n",
      "6         import  202530\n",
      "7        collect  202530\n",
      "8        convert  202530\n",
      "9            sql  202530\n",
      "10   information  202530\n",
      "11       impacts  202530\n",
      "12       testing  202530\n",
      "13     databases  202530\n",
      "14   operational  202530\n",
      "15        python  202530\n",
      "16    analytical  202530\n",
      "17        issues  202530\n",
      "18        trends  202530\n",
      "19       process  202530\n",
      "20     operation  202530\n",
      "21       methods  202530\n",
      "22   maintaining  202530\n",
      "23   performance  202530\n",
      "24       monitor  202530\n",
      "25    management  202530\n",
      "26      collects  202530\n",
      "27      database  202530\n",
      "28    caseworthy  202530\n",
      "29      findings  202530\n",
      "30         clean  202530\n",
      "31     reporting  202530\n",
      "32       tableau  202530\n",
      "33   responsible  202530\n",
      "34    executable  202530\n",
      "35        design  202530\n",
      "36   correlation  202530\n",
      "37      patterns  202530\n",
      "38    dashboards  202530\n",
      "39   statistical  202530\n",
      "40           sas  202530\n",
      "41      insights  202530\n",
      "42      modeling  202530\n",
      "43     organizes  202530\n",
      "44             r  202530\n",
      "45    hypotheses  202530\n",
      "['client', 'stata', 'optimization', 'case', 'development', 'integrity', 'import', 'collect', 'convert', 'sql', 'information', 'impacts', 'testing', 'databases', 'operational', 'python', 'analytical', 'issues', 'trends', 'process', 'operation', 'methods', 'maintaining', 'performance', 'monitor', 'management', 'collects', 'database', 'caseworthy', 'findings', 'clean', 'reporting', 'tableau', 'responsible', 'executable', 'design', 'correlation', 'patterns', 'dashboards', 'statistical', 'sas', 'insights', 'modeling', 'organizes', 'r', 'hypotheses']\n"
     ]
    }
   ],
   "source": [
    "for index, row in id_desc_df.iterrows():\n",
    "    hld_pos_id = row['pos_id']\n",
    "    corpus = []\n",
    "    corpus = row['description']\n",
    "    skills_list = normalize_skills_list(corpus)\n",
    "    load_skills(hld_pos_id, skills_list)\n",
    "    print(skills_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
